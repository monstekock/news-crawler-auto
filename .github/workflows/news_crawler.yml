name: crawl

on:
  workflow_dispatch:        # 수동 실행
  schedule:
    - cron: '0 15 * * *'    # 매일 KST 00:00

jobs:
  run:
    runs-on: self-hosted
    env:
      GOOGLE_APPLICATION_CREDENTIALS_B64: ${{ secrets.GOOGLE_APPLICATION_CREDENTIALS_B64 }}

    steps:
      # 1) 코드 가져오기
      - uses: actions/checkout@v3

      # 2) 의존 패키지 설치 (feedparser 포함)
      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install requests beautifulsoup4 gspread google-auth feedparser

      # 3) GCP 키(Base64) → JSON 복원
      - name: Decode GCP key
        run: |
          echo "$GOOGLE_APPLICATION_CREDENTIALS_B64" | base64 -d > service_account.json

      # 4) 크롤러 실행
      - name: Run crawler
        run: python3 news_crawler_automation.py
