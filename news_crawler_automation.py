#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
news_crawler_automation.py
â–¶ TMZ Â· US Weekly Â· People ê¸°ì‚¬ ë³¸ë¬¸ì„ Google Sheets ë¡œ ì €ìž¥
â–¶ GitHub Actions / self-hosted runnerìš©
"""

import os, json, base64, requests
from datetime import datetime
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘  êµ¬ê¸€ ì¸ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
B64_KEY = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_B64")   # â† ì‹œí¬ë¦¿ ì´ë¦„ ê·¸ëŒ€ë¡œ
if not B64_KEY:
    raise ValueError("âŒ env GOOGLE_APPLICATION_CREDENTIALS_B64 ê°€ ì—†ìŠµë‹ˆë‹¤.")

KEY_PATH = "service_account.json"
with open(KEY_PATH, "wb") as f:
    f.write(base64.b64decode(B64_KEY))

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_file(KEY_PATH, scopes=SCOPES)
client = gspread.authorize(creds)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡ ì‹œíŠ¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SHEET_ID   = "1IBkE0pECiWpF9kLdzEz7-1E-XyRBA02xiVHvwJCwKbc"
SHEET_NAME = "github news room"
sheet      = client.open_by_key(SHEET_ID).worksheet(SHEET_NAME)

def save_to_sheet(rows:list[dict]) -> None:
    """{source,title,date,url,body} dict â†’ ì‹œíŠ¸ append"""
    for r in rows:
        sheet.append_row([r["source"], r["title"], r["date"], r["url"], r["body"]])

def fmt(date_str:str) -> str:
    try:
        return datetime.strptime(date_str, "%B %d, %Y").strftime("%Y-%m-%d")
    except Exception:
        return datetime.utcnow().strftime("%Y-%m-%d")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¢ í¬ë¡¤ëŸ¬ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_tmz(max_cnt:int=5) -> list[dict]:
    """TMZ â€˜Breaking Newsâ€™ ìµœì‹  ê¸°ì‚¬ n ê±´"""
    base = "https://www.tmz.com"
    res  = requests.get(f"{base}/categories/breaking-news/")
    soup = BeautifulSoup(res.text, "html.parser")

    # ðŸ”§ (ìˆ˜ì •) 2025-04 ê¸°ì¤€ ì¹´ë“œ êµ¬ì¡°: <article> â€¦ <a data-analytics-id="CardLink" â€¦>
    links = [
        (base + a["href"]) for a in soup.select('article a[data-analytics-id="CardLink"]')
        if a.get("href", "").startswith("/")
    ][:max_cnt]

    rows=[]
    for url in links:
        s  = BeautifulSoup(requests.get(url).text, "html.parser")
        title = s.select_one("h1.article__headline") or s.select_one("h1")
        body  = s.select_one("div.article__body")    or s.select_one("div.article-content")
        date  = s.select_one("time") or s.select_one("span.publish-date")
        if title and body:
            rows.append({
                "source": "TMZ",
                "title" : title.text.strip(),
                "date"  : fmt(date.text.strip() if date else ""),
                "url"   : url,
                "body"  : body.text.strip().replace("\n", " ")[:3000],
            })
    return rows

def crawl_usweekly(max_cnt:int=3) -> list[dict]:
    base = "https://www.usmagazine.com"
    soup = BeautifulSoup(requests.get(f"{base}/news/").text, "html.parser")
    links = [a["href"] for a in soup.select("a.content-card__link") if a["href"].startswith("https://")][:max_cnt]
    rows=[]
    for url in links:
        s=BeautifulSoup(requests.get(url).text,"html.parser")
        title=s.select_one("h1.post-title")
        body =s.select_one("div.post-content")
        date =s.select_one("time.published-date")
        if title and body:
            rows.append({
                "source":"US Weekly","title":title.text.strip(),
                "date":fmt(date.text.strip() if date else ""),
                "url":url,"body":body.text.strip().replace("\n"," ")[:3000]
            })
    return rows

def crawl_people(max_cnt:int=3) -> list[dict]:
    base="https://people.com"
    soup=BeautifulSoup(requests.get(f"{base}/tag/the-scoop/").text,"html.parser")
    links=[base+a["href"] for a in soup.select('a[data-testid="CardLink"]') if a["href"].startswith("/")][:max_cnt]
    rows=[]
    for url in links:
        s=BeautifulSoup(requests.get(url).text,"html.parser")
        title=s.select_one("h1.headline")
        body =s.select_one("div.article-body")
        date =s.select_one("time.article-date")
        if title and body:
            rows.append({
                "source":"People","title":title.text.strip(),
                "date":fmt(date.text.strip() if date else ""),
                "url":url,"body":body.text.strip().replace("\n"," ")[:3000]
            })
    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘£ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    data = crawl_tmz() + crawl_usweekly() + crawl_people()
    save_to_sheet(data)
    print(f"{len(data)} articles saved.")
